{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](./table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy.random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import uniform \n",
    "from filterpy.monte_carlo import systematic_resample\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randn\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_particles(x_range, y_range, hdg_range, N):\n",
    "    particles = np.empty((N, 3))\n",
    "    particles[:, 0] = uniform(x_range[0], x_range[1], size=N)\n",
    "    particles[:, 1] = uniform(y_range[0], y_range[1], size=N)\n",
    "    particles[:, 2] = uniform(hdg_range[0], hdg_range[1], size=N)\n",
    "    particles[:, 2] %= 2 * np.pi\n",
    "    return particles\n",
    "\n",
    "def create_uniform_particles_quadratic(x_range, a_range, b_range,s_range,N):\n",
    "    particles = np.empty((N, 4))\n",
    "    particles[:, 0] = uniform(x_range[0],x_range[1],size=N)\n",
    "    particles[:, 1] = uniform(a_range[0],a_range[1],size=N)\n",
    "    particles[:, 2] = uniform(b_range[0],b_range[1],size=N)\n",
    "    particles[:, 3] = uniform(s_range[0],s_range[1],size=N)\n",
    "    return particles\n",
    "\n",
    "def create_gaussian_particles(mean, std, N):\n",
    "    particles = np.empty((N, 3))\n",
    "    particles[:, 0] = mean[0] + (randn(N) * std[0])\n",
    "    particles[:, 1] = mean[1] + (randn(N) * std[1])\n",
    "    particles[:, 2] = mean[2] + (randn(N) * std[2])\n",
    "    particles[:, 2] %= 2 * np.pi\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_quadratic(particles_predict, weights, z, R, landmarks):\n",
    "    for i, landmark in enumerate(landmarks):\n",
    "        distance = np.linalg.norm(particles_predict[:, 0] - landmark, axis=0)\n",
    "        weights *= scipy.stats.norm(distance, R).pdf(z[i])\n",
    "\n",
    "    weights += 1.e-300      # avoid round-off to zero\n",
    "    weights /= sum(weights) # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "\n",
    "    pos = particles[:, 0]\n",
    "    mean = np.average(pos, weights=weights)\n",
    "    var  = np.average((pos - mean)**2, weights=weights)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_resample(particles, weights):\n",
    "    N = len(particles)\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1. # avoid round-off error\n",
    "    indexes = np.searchsorted(cumulative_sum, random(N))\n",
    "\n",
    "    # resample according to indexes\n",
    "    particles[:] = particles[indexes]\n",
    "    weights.fill(1.0 / N)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = create_uniform_particles((0,1), (0,1), (0, 5), 1000)\n",
    "weights = np.array([.25]*1000)\n",
    "estimate(particles, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neff(weights):\n",
    "    return 1. / np.sum(np.square(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights[:] = weights[indexes]\n",
    "    weights.fill(1.0 / len(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.939],\n",
       "       [0.94 ],\n",
       "       [0.938],\n",
       "       [0.931],\n",
       "       [0.921],\n",
       "       [0.907],\n",
       "       [0.885],\n",
       "       [0.87 ],\n",
       "       [0.834],\n",
       "       [0.828],\n",
       "       [0.835],\n",
       "       [0.823],\n",
       "       [0.815],\n",
       "       [0.8  ],\n",
       "       [0.8  ],\n",
       "       [0.809],\n",
       "       [0.809],\n",
       "       [0.795],\n",
       "       [0.776],\n",
       "       [0.766],\n",
       "       [0.733],\n",
       "       [0.726],\n",
       "       [0.682],\n",
       "       [0.678],\n",
       "       [0.7  ],\n",
       "       [0.732],\n",
       "       [0.772],\n",
       "       [0.815],\n",
       "       [0.879],\n",
       "       [0.9  ],\n",
       "       [0.902],\n",
       "       [0.832],\n",
       "       [0.818],\n",
       "       [0.794],\n",
       "       [0.766],\n",
       "       [0.728],\n",
       "       [0.677],\n",
       "       [0.63 ],\n",
       "       [0.519],\n",
       "       [0.445],\n",
       "       [0.388],\n",
       "       [0.414],\n",
       "       [0.409],\n",
       "       [0.416],\n",
       "       [0.427],\n",
       "       [0.457],\n",
       "       [0.478],\n",
       "       [0.482],\n",
       "       [0.495],\n",
       "       [0.499],\n",
       "       [0.512],\n",
       "       [0.516],\n",
       "       [0.495],\n",
       "       [0.446],\n",
       "       [0.416],\n",
       "       [0.323],\n",
       "       [0.258],\n",
       "       [0.201],\n",
       "       [0.188],\n",
       "       [0.225],\n",
       "       [0.3  ],\n",
       "       [0.319],\n",
       "       [0.334],\n",
       "       [0.349],\n",
       "       [0.308],\n",
       "       [0.312],\n",
       "       [0.324],\n",
       "       [0.352],\n",
       "       [0.383],\n",
       "       [0.4  ],\n",
       "       [0.354],\n",
       "       [0.312],\n",
       "       [0.256],\n",
       "       [0.246],\n",
       "       [0.258],\n",
       "       [0.286],\n",
       "       [0.321],\n",
       "       [0.384],\n",
       "       [0.43 ],\n",
       "       [0.445],\n",
       "       [0.431],\n",
       "       [0.398],\n",
       "       [0.408],\n",
       "       [0.401],\n",
       "       [0.402]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "import scipy.io as sio\n",
    "mat_contents = sio.loadmat('measurement1.mat')\n",
    "sorted(mat_contents.keys())\n",
    "landmarks_all = mat_contents['PF_measurement']\n",
    "landmarks = landmarks_all[0:85,:]\n",
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pf1(N, sensor_std_err=.1, \n",
    "            do_plot=True, plot_particles=False,\n",
    "            xlim=(0, 200), ylim=(0, 1),\n",
    "            initial_x=None):\n",
    "    \n",
    "    NL = len(landmarks) # k1\n",
    "    print(NL)\n",
    "    #plt.figure()\n",
    "   \n",
    "    # create particles and weights\n",
    "    particles = create_uniform_particles_quadratic((0.9,1.1),(0.0,0.0001),(-0.1,0.0),(0.1,1),N)\n",
    "    \n",
    "    weights = np.ones(N) / N\n",
    "    \n",
    "    \n",
    "    if plot_particles:\n",
    "        alpha = .20\n",
    "        if N > 5000:\n",
    "            alpha *= np.sqrt(5000)/np.sqrt(N)           \n",
    "        plt.scatter(particles[:, 0], particles[:, 1], \n",
    "                    alpha=alpha, color='g')\n",
    "    \n",
    "    xs = []\n",
    "    #robot_pos = np.array([0., 0.])\n",
    "    threshold=0.26\n",
    "    k=1\n",
    "    \n",
    "    while min(particles[:,0]) > threshold:\n",
    "        k += 1\n",
    "        particles_predict = particles\n",
    "        \n",
    "        particles_predict[:,0] += 2 * particles_predict[:,1] * (k-1) + particles_predict[:,1] + particles_predict[:,2]\n",
    "    \n",
    "        if k < NL:\n",
    "            zs = (norm(landmarks - particles_predict[:,0], axis=1) + \n",
    "              (randn(NL) * sensor_std_err))\n",
    "            print(zs[1:10])\n",
    "            update_quadratic(particles_predict, weights, z=zs, R=sensor_std_err, \n",
    "               landmarks=landmarks)\n",
    "            print(\"neff\", neff(weights))\n",
    "            if neff(weights) < N/2:\n",
    "                print(indexes)\n",
    "                indexes = simple_resample(weights)\n",
    "                print(indexes)\n",
    "                #indexes = systematic_resample(weights)\n",
    "                weights = resample_from_index(particles, weights, indexes)\n",
    "                assert np.allclose(weights, 1/N)\n",
    "            print(particles[1,0])\n",
    "            print(weights[1])\n",
    "            mu, var = estimate(particles, weights)\n",
    "            xs.append(mu)\n",
    "            \n",
    "        else:\n",
    "            particles = particles_predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "[4.834 4.717 4.594 4.995 5.578 6.601 7.282 9.557 9.896]\n",
      "neff 5000.000000000623\n",
      "0.8467915528704002\n",
      "0.00019999999999998757\n",
      "[6.467 6.656 6.211 6.117 5.717 5.98  6.156 7.481 7.968]\n",
      "neff 5000.000000000289\n",
      "0.7483461760790219\n",
      "0.0001999999999999942\n",
      "[9.824 9.744 9.306 8.826 8.292 7.745 7.586 7.578 7.778]\n",
      "neff 4999.9999999997235\n",
      "0.6499733538522097\n",
      "0.00020000000000000554\n",
      "[13.543 13.509 13.083 12.52  11.814 10.963 10.494  9.657  9.41 ]\n",
      "neff 5000.000000000789\n",
      "0.5516730861899635\n",
      "0.0001999999999999842\n",
      "[17.283 17.265 16.844 16.225 15.686 14.527 13.858 12.378 12.282]\n",
      "neff 4999.999999999471\n",
      "0.4534453730922834\n",
      "0.0002000000000000106\n",
      "[21.347 20.997 20.732 20.333 19.484 18.414 17.697 15.998 15.752]\n",
      "neff 5000.000000000154\n",
      "0.35529021455916937\n",
      "0.00019999999999999692\n",
      "[25.243 25.067 24.795 24.227 23.403 22.417 21.446 19.728 19.309]\n",
      "neff 5000.000000000521\n",
      "0.2572076105906214\n",
      "0.00019999999999998957\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#     for x in range(iters):\n",
    "#         robot_pos += (1, 1)\n",
    "\n",
    "#         # distance from robot to each landmark\n",
    "#         zs = (norm(landmarks - robot_pos, axis=1) + \n",
    "#               (randn(NL) * sensor_std_err))\n",
    "\n",
    "#         # move diagonally forward to (x+1, x+1)\n",
    "#         predict(particles, u=(0.00, 1.414), std=(.2, .05))\n",
    "        \n",
    "#         # incorporate measurements\n",
    "#         update(particles, weights, z=zs, R=sensor_std_err, \n",
    "#                landmarks=landmarks)\n",
    "        \n",
    "#         # resample if too few effective particles\n",
    "#         if neff(weights) < N/2:\n",
    "#             indexes = systematic_resample(weights)\n",
    "#             resample_from_index(particles, weights, indexes)\n",
    "#             assert np.allclose(weights, 1/N)\n",
    "#         mu, var = estimate(particles, weights)\n",
    "#         xs.append(mu)\n",
    "\n",
    "#         if plot_particles:\n",
    "#             plt.scatter(particles[:, 0], particles[:, 1], \n",
    "#                         color='k', marker=',', s=1)\n",
    "#         p1 = plt.scatter(robot_pos[0], robot_pos[1], marker='+',\n",
    "#                          color='k', s=180, lw=3)\n",
    "#         p2 = plt.scatter(mu[0], mu[1], marker='s', color='r')\n",
    "    \n",
    "#     xs = np.array(xs)\n",
    "#     #plt.plot(xs[:, 0], xs[:, 1])\n",
    "#     plt.legend([p1, p2], ['Actual', 'PF'], loc=4, numpoints=1)\n",
    "#     plt.xlim(*xlim)\n",
    "#     plt.ylim(*ylim)\n",
    "#     print('final position error, variance:\\n\\t', mu - np.array([iters, iters]), var)\n",
    "#     plt.show()\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(200) \n",
    "run_pf1(N=5000, plot_particles=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of this code is devoted to initialization and plotting. The entirety of the particle filter processing consists of these lines:\n",
    "\n",
    "```python\n",
    "# move diagonally forward to (x+1, x+1)\n",
    "predict(particles, u=(0.00, 1.414), std=(.2, .05))\n",
    "\n",
    " # incorporate measurements\n",
    "update(particles, weights, z=zs, R=sensor_std_err, \n",
    "       landmarks=landmarks)\n",
    "       \n",
    "# resample if too few effective particles\n",
    "if neff(weights) < N/2:\n",
    "    indexes = systematic_resample(weights)\n",
    "    resample_from_index(particles, weights, indexes)\n",
    "\n",
    "mu, var = estimate(particles, weights)\n",
    "```\n",
    "\n",
    "The first line predicts the position of the particles with the assumption that the robot is moving in a straight line (`u[0] == 0`) and moving 1 unit in both the x and y axis (`u[1]==1.414`). The standard deviation for the error in the turn is 0.2, and the standard deviation for the distance is 0.05. When this call returns the particles will all have been moved forward, but the weights are no longer correct as they have not been updated.\n",
    "\n",
    "The next line incorporates the measurement into the filter. This does not alter the particle positions, it only alters the weights. If you recall the weight of the particle is computed as the probability that it matches the Gaussian of the sensor error model. The further the particle from the measured distance the less likely it is to be a good representation.\n",
    "\n",
    "The final two lines example the effective particle count ($\\hat{N}_\\text{eff})$. If it falls below $N/2$ we perform resampling to try to ensure our particles form a good representation of the actual probability distribution.\n",
    "\n",
    "Now let's look at this with all the particles plotted. Seeing this happen interactively is much more instructive, but this format still gives us useful information. I plotted the original random distribution of points in a very pale green and large circles to help distinguish them from the subsequent iterations where the particles are plotted with black pixels. The number of particles makes it hard to see the details, so I limited the number of iterations to 8 so we can zoom in and look more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2)\n",
    "run_pf1(N=5000, iters=8, plot_particles=True, \n",
    "        xlim=(0,8), ylim=(0,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it looks like there are only a few particles at the first two robot positions. This is not true; there are 5,000 particles, but due to resampling most are duplicates of each other. The reason for this is the Gaussian for the sensor is very narrow. This is called *sample impoverishment* and can lead to filter divergence. I'll address this in detail below. For now, looking at the second step at x=2 we can see that the particles have dispersed a bit. This dispersion is due to the motion model noise. All particles are projected forward according to the control input `u`, but noise is added to each particle proportional to the error in the control mechanism in the robot. By the third step the particles have dispersed enough to make a convincing cloud of particles around the robot. \n",
    "\n",
    "The shape of the particle cloud is an ellipse. This is not a coincidence. The sensors and robot control are both modeled as Gaussian, so the probability distribution of the system is also a Gaussian. The particle filter is a sampling of the probability distribution, so the cloud should be an ellipse.\n",
    "\n",
    "It is important to recognize that the particle filter algorithm *does not require* the sensors or system to be Gaussian or linear. Because we represent the probability distribution with a cloud of particles we can handle any probability distribution and strongly nonlinear problems. There can be discontinuities and hard limits in the probability model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Sensor Errors on the Filter\n",
    "\n",
    "The first few iterations of the filter resulted in many duplicate particles. This happens because the model for the sensors is Gaussian, and we gave it a small standard deviation of $\\sigma=0.1$. This is  counterintuitive at first. The Kalman filter performs better when the noise is smaller, yet the particle filter can perform worse.\n",
    "\n",
    "\n",
    "We can reason about why this is true. If $\\sigma=0.1$, the robot is at (1, 1) and a particle is at (2, 2) the particle is 14 standard deviations away from the robot. This gives it a near zero probability. It contributes nothing to the estimate of the mean, and it is extremely unlikely to survive after the resampling. If $\\sigma=1.4$ then the particle is only $1\\sigma$ away and thus it will contribute to the estimate of the mean. During resampling it is likely to be copied one or more times.\n",
    "\n",
    "This is *very important* to understand - a very accurate sensor can lead to poor performance of the filter because few of the particles will be a good sample of the probability distribution. There are a few fixes available to us. First, we can artificially increase the sensor noise standard deviation so the particle filter will accept more points as matching the robots probability distribution. This is non-optimal because some of those points will be a poor match. The real problem is that there aren't enough points being generated such that enough are near the robot. Increasing `N` usually fixes this problem. This decision is not cost free as increasing the number of particles significantly increase the computation time. Still, let's look at the result of using 100,000 particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed(2) \n",
    "run_pf1(N=100000, iters=8, plot_particles=True, \n",
    "        xlim=(0,8), ylim=(0,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more particles at x=1, and we have a convincing cloud at x=2. Clearly the filter is performing better, but at the cost of large memory usage and long run times.\n",
    "\n",
    "Another approach is to be smarter about generating the initial particle cloud. Suppose we guess that the robot is near (0, 0). This is not exact, as the simulation actually places the robot at (1, 1), but it is close. If we create a normally distributed cloud near (0, 0) there is a much greater chance of the particles matching the robot's position.\n",
    "\n",
    "`run_pf1()` has an optional parameter `initial_x`. Use this to specify the initial position guess for the robot. The code then uses `create_gaussian_particles(mean, std, N)` to create particles distributed normally around the initial guess. We will use this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Degeneracy From Inadequate Samples\n",
    "\n",
    "The filter as written is far from perfect. Here is how it performs with a different random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(6) \n",
    "run_pf1(N=5000, plot_particles=True, ylim=(-20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the initial sample of points did not generate any points near the robot. The particle filter does not create new points during the resample operation, so it ends up duplicating points which are not a representative sample of the probability distribution. As mentioned earlier this is called *sample impoverishment*. The problem quickly spirals out of control. The particles are not a good match for the landscape measurement so they become dispersed in a highly nonlinear, curved distribution, and the particle filter diverges from reality. No particles are available near the robot, so it cannot ever converge.\n",
    "\n",
    "Let's make use of the `create_gaussian_particles()` method to try to generate more points near the robot. We can do this by using the `initial_x` parameter to specify a location to create the particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(6) \n",
    "run_pf1(N=5000, plot_particles=True, initial_x=(1,1, np.pi/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works great. You should always try to create particles near the initial position if you have any way to roughly estimate it. Do not be *too* careful - if you generate all the points very near a single position the particles may not be dispersed enough to capture the nonlinearities in the system. This is a fairly linear system, so we could get away with a smaller variance in the distribution. Clearly this depends on your problem. Increasing the number of particles is always a good way to get a better sample, but the processing cost may be a higher price than you are willing to pay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n",
    "\n",
    "I've hand waved a difficulty away which we must now confront. There is some probability distribution that describes the position and movement of our robot. We want to draw a sample of particles from that distribution and compute the integral using MC methods. \n",
    "\n",
    "Our difficulty is that in many problems we don't know the distribution. For example, the tracked object might move very differently than we predicted with our state model. How can we draw a sample from a probability distribution that is unknown? \n",
    "\n",
    "There is a theorem from statistics called [*importance sampling*](https://en.wikipedia.org/wiki/Importance_sampling)[1]. Remarkably, it gives us a way to draw samples from a different and known probability distribution and use those to compute the properties of the unknown one. It's a fantastic theorem that brings joy to my heart. \n",
    "\n",
    "The idea is simple, and we already used it. We draw samples from the known probability distribution, but *weight the samples* according to the distribution we are interested in. We can then compute properties such as the mean and variance by computing the weighted mean and weighted variance of the samples.\n",
    "\n",
    "For the robot localization problem we drew samples from the probability distribution that we computed from our state model prediction step. In other words, we reasoned 'the robot was there, it is perhaps moving at this direction and speed, hence it might be here'. Yet the robot might have done something completely different. It may have fell off a cliff or been hit by a mortar round. In each case the probability distribution is not correct. It seems like we are stymied, but we are not because we can use importance sampling. We drew particles from that likely incorrect probability distribution, then weighted them according to how well the particles match the measurements. That weighting is based on the true probability distribution, so according to the theory the resulting mean, variance, etc, will be correct!\n",
    "\n",
    "How can that be true? I'll give you the math; you can safely skip this if you don't plan to go beyond the robot localization problem. However, other particle filter problems require different approaches to importance sampling, and a bit of math helps. Also, the literature and much of the content on the web uses the mathematical formulation in favor of my rather imprecise \"imagine that...\" exposition. If you want to understand the literature you will need to know the following equations.\n",
    "\n",
    "We have some probability distribution $\\pi(x)$ which we want to take samples from. However, we don't know what $\\pi(x)$ is; instead we only know an alternative probability distribution $q(x)$. In the context of robot localization, $\\pi(x)$ is the probability distribution for the robot, but we don't know it, and $q(x)$ is the probability distribution of our measurements, which we do know.\n",
    "\n",
    "The expected value of a function $f(x)$ with probability distribution $\\pi(x)$ is\n",
    "\n",
    "$$\\mathbb{E}\\big[f(x)\\big] = \\int f(x)\\pi(x)\\, dx$$\n",
    "\n",
    "We don't know $\\pi(x)$ so we cannot compute this integral. We do know an alternative distribution $q(x)$ so we can add it into the integral without changing the value with\n",
    "\n",
    "$$\\mathbb{E}\\big[f(x)\\big] = \\int f(x)\\pi(x)\\frac{q(x)}{q(x)}\\, dx$$\n",
    "\n",
    "Now we rearrange and group terms\n",
    "\n",
    "$$\\mathbb{E}\\big[f(x)\\big] = \\int f(x)q(x)\\, \\,  \\cdot \\,  \\frac{\\pi(x)}{q(x)}\\, dx$$\n",
    "\n",
    "$q(x)$ is known to us, so we can compute $\\int f(x)q(x)$ using MC integration. That leaves us with  $\\pi(x)/q(x)$.  That is a ratio, and we define it as a *weight*. This gives us\n",
    "\n",
    "$$\\mathbb{E}\\big[f(x)\\big] = \\sum\\limits_{i=1}^N f(x^i)w(x^i)$$\n",
    "\n",
    "Maybe that seems a little abstract. If we want to compute the mean of the particles we would compute\n",
    "\n",
    "$$\\mu = \\sum\\limits_{i=1}^N x^iw^i$$\n",
    "\n",
    "which is the equation I gave you earlier in the chapter.\n",
    "\n",
    "It is required that the weights be proportional to the ratio $\\pi(x)/q(x)$. We normally do not know the exact value, so in practice we normalize the weights by dividing them by $\\sum w(x^i)$.\n",
    "\n",
    "When you formulate a particle filter algorithm you will have to implement this step depending on the particulars of your situation. For robot localization the best distribution to use for $q(x)$ is the particle distribution from the `predict()` step of the filter. Let's look at the code again:\n",
    "\n",
    "```python\n",
    "def update(particles, weights, z, R, landmarks):\n",
    "    for i, landmark in enumerate(landmarks):\n",
    "        dist = np.linalg.norm(particles[:, 0:2] - landmark, axis=1)\n",
    "        weights *= scipy.stats.norm(dist, R).pdf(z[i])\n",
    "\n",
    "    weights += 1.e-300      # avoid round-off to zero\n",
    "    weights /= sum(weights) # normalize\n",
    "```\n",
    "   \n",
    "Here we compute the weight as the based on the Bayesian computation $\\| \\text{likelihood} \\times \\text{prior}\\|$\n",
    "\n",
    "Of course if you can compute the posterior probability distribution from the prior you should do so. If you cannot, then importance sampling gives you a way to solve this problem. In practice, computing the posterior is incredibly difficult. The Kalman filter became a spectacular success because it took advantage of the properties of Gaussians to find an analytic solution. Once we relax the conditions required by the Kalman filter (Markov property, Gaussian measurements and process) importance sampling and monte carlo methods make the problem tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Methods\n",
    "\n",
    "The resampling algorithm affects the performance of the filter. For example, suppose we resampled particles by picking particles at random. This would lead us to choosing many particles with a very low weight, and the resulting set of particles would be a terrible representation of the problem's probability distribution. \n",
    "\n",
    "Research on the topic continues, but a handful of algorithms work well in practice across a wide variety of situations. We desire an algorithm that has several properties. It should preferentially select particles that have a higher probability. It should select a representative population of the higher probability particles to avoid sample impoverishment. It should include enough lower probability particles to give the filter a chance of detecting strongly nonlinear behavior. \n",
    "\n",
    "FilterPy implements several of the popular algorithms. FilterPy doesn't know how your particle filter is implemented, so it cannot generate the new samples. Instead, the algorithms create a `numpy.array` containing the indexes of the particles that are chosen. Your code needs to perform the resampling step. For example, I used this for the robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights[:] = weights[indexes]\n",
    "    weights.fill(1.0 / len(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Resampling\n",
    "\n",
    "Multinomial resampling is the algorithm that I used while developing the robot localization example. The idea is simple. Compute the cumulative sum of the normalized weights. This gives you an array of increasing values from 0 to 1. Here is a plot which illustrates how this spaces out the weights. The colors are meaningless, they just make the divisions easier to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from kf_book.pf_internal import plot_cumsum\n",
    "print('cumulative sume is', np.cumsum([.1, .2, .1, .6]))\n",
    "plot_cumsum([.1, .2, .1, .6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a weight we generate a random number uniformly selected between 0 and 1 and use binary search to find its position inside the cumulative sum array. Large weights occupy more space than low weights, so they are more likely to be selected. \n",
    "\n",
    "This is very easy to code using NumPy's [ufunc](http://docs.scipy.org/doc/numpy/reference/ufuncs.html) support. Ufuncs apply functions to every element of an array, returning an array of the results. `searchsorted` is NumPy's binary search algorithm. If you provide it with an array of search values it will return an array of answers: a single answer for each search value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomal_resample(weights):\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1.  # avoid round-off errors\n",
    "    return np.searchsorted(cumulative_sum, random(len(weights)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kf_book.pf_internal import plot_multinomial_resample\n",
    "plot_multinomial_resample([.1, .2, .3, .4, .2, .3, .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an $O(n \\log(n))$ algorithm. That is not terrible, but there are $O(n)$ resampling algorithms with better properties with respect to the uniformity of the samples. I'm showing it because you can understand the other algorithms as variations on this one. There is a faster implementation of this multinomial resampling that uses the inverse of the CDF of the distribution. You can search on the internet if you are interested.\n",
    "\n",
    "Import the function from FilterPy using\n",
    "\n",
    "```python\n",
    "from filterpy.monte_carlo import multinomal_resample\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Resampling\n",
    "\n",
    "Residual resampling both improves the run time of multinomial resampling, and ensures that the sampling is uniform across the population of particles. It's fairly ingenious: the normalized weights are multiplied by *N*, and then the integer value of each weight is used to define how many samples of that particle will be taken. For example, if the weight of a particle is 0.0012 and $N$=3000, the scaled weight is 3.6, so 3 samples will be taken of that particle. This ensures that all higher weight particles are chosen at least once. The running time is $O(N)$, making it faster than multinomial resampling.\n",
    "\n",
    "However, this does not generate all *N* selections. To select the rest, we take the *residual*: the weights minus the integer part, which leaves the fractional part of the number. We then use a simpler sampling scheme such as multinomial, to select the rest of the particles based on the residual. In the example above the scaled weight was 3.6, so the residual will be 0.6 (3.6 - int(3.6)). This residual is very large so the particle will be likely to be sampled again. This is reasonable because the larger the residual the larger the error in the round off, and thus the particle was relatively under sampled in the integer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_resample(weights):\n",
    "    N = len(weights)\n",
    "    indexes = np.zeros(N, 'i')\n",
    "\n",
    "    # take int(N*w) copies of each weight\n",
    "    num_copies = (N*np.asarray(weights)).astype(int)\n",
    "    k = 0\n",
    "    for i in range(N):\n",
    "        for _ in range(num_copies[i]): # make n copies\n",
    "            indexes[k] = i\n",
    "            k += 1\n",
    "\n",
    "    # use multinormial resample on the residual to fill up the rest.\n",
    "    residual = w - num_copies     # get fractional part\n",
    "    residual /= sum(residual)     # normalize\n",
    "    cumulative_sum = np.cumsum(residual)\n",
    "    cumulative_sum[-1] = 1. # ensures sum is exactly one\n",
    "    indexes[k:N] = np.searchsorted(cumulative_sum, random(N-k))\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be tempted to replace the inner for loop with a slice `indexes[k:k + num_copies[i]] = i`, but very short slices are comparatively slow, and the for loop usually runs faster.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kf_book.pf_internal import plot_residual_resample\n",
    "plot_residual_resample([.1, .2, .3, .4, .2, .3, .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may import this from FilterPy using\n",
    "\n",
    "```python\n",
    "    from filterpy.monte_carlo import residual_resample\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Resampling\n",
    "\n",
    "This scheme aims to make selections relatively uniformly across the particles. It works by dividing the cumulative sum into $N$ equal sections, and then selects one particle randomly from each section.  This guarantees that each sample is between 0 and $\\frac{2}{N}$ apart.\n",
    "\n",
    "The plot below illustrates this. The colored bars show the cumulative sum of the array, and the black lines show the $N$ equal subdivisions. Particles, shown as black circles, are randomly placed in each subdivision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kf_book.pf_internal import plot_stratified_resample\n",
    "plot_stratified_resample([.1, .2, .3, .4, .2, .3, .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to perform the stratification is quite straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_resample(weights):\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, chose a random position within each one\n",
    "    positions = (random(N) + range(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import it from FilterPy with\n",
    "\n",
    "```python\n",
    "from filterpy.monte_carlo import stratified_resample\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Resampling\n",
    "\n",
    "The last algorithm we will look at is systemic resampling. As with stratified resampling the space is divided into $N$ divisions. We then choose a random offset to use for all of the divisions, ensuring that each sample is exactly $\\frac{1}{N}$ apart. It looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kf_book.pf_internal import plot_systematic_resample\n",
    "plot_systematic_resample([.1, .2, .3, .4, .2, .3, .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having seen the earlier examples the code couldn't be simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_resample(weights):\n",
    "    N = len(weights)\n",
    "\n",
    "    # make N subdivisions, choose positions \n",
    "    # with a consistent random offset\n",
    "    positions = (np.arange(N) + random()) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "Import from FilterPy with\n",
    "\n",
    "```python\n",
    "from filterpy.monte_carlo import systematic_resample\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Resampling Algorithm\n",
    "\n",
    "Let's look at the four algorithms at once so they are easier to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = [.1, .2, .3, .4, .2, .3, .1]\n",
    "np.random.seed(4)\n",
    "plot_multinomial_resample(a)\n",
    "plot_residual_resample(a)\n",
    "plot_systematic_resample(a)\n",
    "plot_stratified_resample(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the multinomial resampling is quite bad. There is a very large weight that was not sampled at all. The largest weight only got one resample, yet the smallest weight was sample was sampled twice. Most tutorials on the net that I have read use multinomial resampling, and I am not sure why. Multinomial resampling is rarely used in the literature or for real problems. I recommend not using it unless you have a very good reason to do so.\n",
    "\n",
    "The residual resampling algorithm does excellently at what it tries to do: ensure all the largest weights are resampled multiple times. It doesn't evenly distribute the samples across the particles - many reasonably large weights are not resampled at all. \n",
    "\n",
    "Both systematic and stratified perform very well. Systematic sampling does an excellent job of ensuring we sample from all parts of the particle space while ensuring larger weights are proportionality resampled more often. Stratified resampling is not quite as uniform as systematic resampling, but it is a bit better at ensuring the higher weights get resampled more.\n",
    "\n",
    "Plenty has been written on the theoretical performance of these algorithms, and feel free to read it.  In practice I apply particle filters to problems that resist analytic efforts, and so I am a bit dubious about the validity of a specific analysis to these problems. In practice both the stratified and systematic algorithms perform well and similarly across a variety of problems. I say try one, and if it works stick with it. If performance of the filter is critical try both, and perhaps see if there is literature published on your specific problem that will give you better guidance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter only touches the surface of what is a vast topic. My goal was not to teach you the field, but to expose you to practical Bayesian Monte Carlo techniques for filtering. \n",
    "\n",
    "Particle filters are a type of *ensemble* filtering. Kalman filters represents state with a Gaussian. Measurements are applied to the Gaussian using Bayes Theorem, and the prediction is done using state-space methods. These techniques are applied to the Gaussian - the probability distribution.\n",
    "\n",
    "In contrast, ensemble techniques represent a probability distribution using a discrete collection of points and associated probabilities. Measurements are applied to these points, not the Gaussian distribution. Likewise, the system model is applied to the points, not a Gaussian. We then compute the statistical properties of the resulting ensemble of points.\n",
    "\n",
    "These choices have many trade-offs. The Kalman filter is very efficient, and is an optimal estimator if the assumptions of linearity and Gaussian noise are true. If the problem is nonlinear than we must linearize the problem. If the problem is multimodal (more than one object being tracked) then the Kalman filter cannot represent it. The Kalman filter requires that you know the state model. If you do not know how your system behaves the performance is poor.\n",
    "\n",
    "In contrast, particle filters work with any arbitrary, non-analytic probability distribution. The ensemble of particles, if large enough, form an accurate approximation of the distribution. It performs wonderfully even in the presence of severe nonlinearities. Importance sampling allows us to compute probabilities even if we do not know the underlying probability distribution. Monte Carlo techniques replace the analytic integrals required by the other filters. \n",
    "\n",
    "This power comes with a cost. The most obvious costs are the high computational and memory burdens the filter places on the computer. Less obvious is the fact that they are fickle. You have to be careful to avoid particle degeneracy and divergence. It can be very difficult to prove the correctness of your filter. If you are working with multimodal distributions you have further work to cluster the particles to determine the paths of the multiple objects. This can be very difficult when the objects are close to each other.\n",
    "\n",
    "There are many different classes of particle filter; I only described the naive SIS algorithm, and followed that with a SIR algorithm that performs well. There are many classes of filters, and many examples of filters in each class. It would take a small book to describe them all. \n",
    "\n",
    "When you read the literature on particle filters you will find that it is strewn with integrals. We perform computations on probability distributions using integrals, so using integrals gives the authors a powerful and compact notation. You must recognize that when you reduce these equations to code you will be representing the distributions with particles, and integrations are replaced with sums over the particles. If you keep in mind the core ideas in this chapter the material shouldn't be daunting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] *Importance Sampling*, Wikipedia.\n",
    "https://en.wikipedia.org/wiki/Importance_sampling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
